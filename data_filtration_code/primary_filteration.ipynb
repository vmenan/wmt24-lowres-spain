{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import joblib\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone idiomata_cognitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"idiomata_cognitor/model.pkl\"):\n",
    "    if not os.path.exists(\"idiomata_cognitor\"):\n",
    "        os.system('git clone https://github.com/transducens/idiomata_cognitor.git')\n",
    "    with zipfile.ZipFile(\"idiomata_cognitor/model.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(\"idiomata_cognitor\")\n",
    "else:\n",
    "    print(\"Idiomata_cognitor already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load model for language classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = { \n",
    "    1.: 'Spanish',\n",
    "    2.: 'Catalan',\n",
    "    3.: 'Aragonese',\n",
    "    4.: 'Aranese',\n",
    "    5.: 'Occitan',\n",
    "    6.: 'Asturian',\n",
    "    7.: 'Galician',\n",
    "    8.: 'Italian',\n",
    "    9.: 'French',\n",
    "    10.: 'Portuguese'\n",
    "}\n",
    "clf2 = joblib.load(\"idiomata_cognitor/model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(src_path, trg_path):\n",
    "    with open(src_path, 'r', encoding='utf-8') as file:\n",
    "        input_texts = file.readlines()\n",
    "        input_texts = [line.strip() for line in input_texts]\n",
    "    with open(trg_path, 'r', encoding='utf-8') as file:\n",
    "        references = file.readlines()\n",
    "        references = [line.strip() for line in references]\n",
    "    return input_texts, references\n",
    "\n",
    "def remove_punctuations_and_spaces(text: str) -> str:\n",
    "    # Define the regular expression pattern to match punctuation\n",
    "    punctuation_pattern = r'[!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~]'\n",
    "    \n",
    "    # Remove punctuation from the text\n",
    "    text_without_punctuations = re.sub(punctuation_pattern, '', text)\n",
    "    \n",
    "    # Define the regular expression pattern to match spaces\n",
    "    spaces_pattern = r'\\s+'\n",
    "    \n",
    "    # Remove spaces from the text\n",
    "    cleaned_text = re.sub(spaces_pattern, '', text_without_punctuations).lower()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def deduplicate_df(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    df[f\"processed_{col}\"] = df[col].apply(remove_punctuations_and_spaces)\n",
    "    df.drop_duplicates(subset=[col],keep=\"first\",inplace=True)\n",
    "    df.drop(labels=[f\"processed_{col}\"], inplace=True, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "def language_filteration(df: pd.DataFrame, cols: list, lang_ids: list, step: int = 10_000) -> pd.DataFrame:\n",
    "    preds = {}\n",
    "    for col in cols:\n",
    "        preds[col] = []\n",
    "    for start_idx in tqdm(range(0, len(df), step)):\n",
    "        for col in cols:\n",
    "            preds[col].extend(clf2.predict(df[col][start_idx: start_idx + step].values.astype('U')))\n",
    "    \n",
    "    mask = pd.Series([True] * len(df))\n",
    "    for i, (col, pred_ids) in enumerate(preds.items()):\n",
    "        lang_id = lang_ids[i]\n",
    "        mask &= (pd.Series(pred_ids) == lang_id)\n",
    "    filtered_df = df[mask]\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def dev_stat_filteration(df: pd.DataFrame, df_dev: pd.DataFrame, src_col: str, trg_col: str) -> pd.DataFrame:\n",
    "    df['src_len'] = df[src_col].apply(lambda x: len(x.split()))\n",
    "    df['trg_len'] = df[trg_col].apply(lambda x: len(x.split()))\n",
    "    df['len_ratio'] = df['src_len'] / df['trg_len']\n",
    "    df_dev['src_len'] = df_dev['src'].apply(lambda x: len(x.split()))\n",
    "\n",
    "    # NOTE: The df_dev len_ratio was predetermined and adjusted to give room for error.\n",
    "    filtered_df = df[(df[\"src_len\"] < df_dev['src_len'].max()) & (df[\"len_ratio\"] >= 0.7) & (df[\"len_ratio\"] <= 1.9)]\n",
    "    filtered_df.drop(labels=[\"src_len\", \"trg_len\", \"len_ratio\"], inplace=True, axis=1)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def primary_filteration(df: pd.DataFrame, df_dev: pd.DataFrame, cols: list, lang_ids: list) -> pd.DataFrame:\n",
    "    print(\"Deduplicating...\")\n",
    "    df1 = deduplicate_df(df, cols[0])\n",
    "    print(\"Performing language filteration...\")\n",
    "    df2 = language_filteration(df1, cols, lang_ids)\n",
    "    print(\"Performing statistical filteration...\")\n",
    "    df3 = dev_stat_filteration(df2, df_dev, cols[0], cols[1])\n",
    "    print(\"Done!\")\n",
    "\n",
    "    return df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_dev_path = 'data/dev/dev.spa_Latn'\n",
    "oc_dev_path = 'data/dev/dev.ast_Latn'\n",
    "src_text, trg_text = read_data(es_dev_path, oc_dev_path)\n",
    "df_dev = pd.DataFrame(data={\"src\": src_text, \"trg\": trg_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wikimedia = pd.read_csv(\"es-ast/wikimedia/wikimedia.csv\")\n",
    "print(\"Start size:\", len(df_wikimedia))\n",
    "filtered_df_wikimedia = primary_filteration(df_wikimedia, df_dev, [\"es\", \"ast\"], [1, 6])\n",
    "print(\"Final size:\", len(filtered_df_wikimedia))\n",
    "filtered_df_wikimedia.to_csv(\"filtered_wikimedia.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nllb = pd.read_csv(\"es-ast/nllb/nllb.csv\")\n",
    "df_nllb = df_nllb.astype({'es': 'str', \"ast\": \"str\"})\n",
    "print(\"Start size:\", len(df_nllb))\n",
    "filtered_df_nllb = primary_filteration(df_nllb, df_dev, [\"es\", \"ast\"], [1, 6])\n",
    "print(\"Final size:\", len(filtered_df_nllb))\n",
    "filtered_df_nllb.to_csv(\"filtered_nllb.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

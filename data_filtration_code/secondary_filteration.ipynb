{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5012375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "import re\n",
    "import typing as T\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edd9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "046e8d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding the stopwords for the spanish language\n",
    "STOPWORDS = stopwords.words('spanish') + list(string.punctuation)\n",
    "#setting the number of batches need to be sampled randomly and the\n",
    "#size of each batch\n",
    "num_batches = \"<ENTER NUMBER>\"\n",
    "batch_size = \"<ENTER NUMBER>\"\n",
    "#set the final data size we want to have\n",
    "target_size=50_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0268c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the source for the following functions is the following link: https://github.com/anthonyprinaldi/js-divergence/tree/master\n",
    "\n",
    "def isNumber(num: str) -> bool:\n",
    "    \"\"\"Check if a string is a number\n",
    "\n",
    "    Args:\n",
    "        num (str): piece of text\n",
    "\n",
    "    Returns:\n",
    "        bool: True iff the string can be converted\n",
    "            to a number\n",
    "    \"\"\"\n",
    "    try:\n",
    "        float(num)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "def computeFreqDistribution(doc: str, stopwords: bool = False) -> nltk.FreqDist:\n",
    "    \"\"\"Computes the frequency of each word in a document\n",
    "\n",
    "    Args:\n",
    "        doc (str): string containing the entire document\n",
    "        stopwords (bool): boolean flag indicating whether or not\n",
    "            to remove the stopwords from the sentence. True indicates\n",
    "            to remove the stopwords.\n",
    "\n",
    "    Returns:\n",
    "        nltk.FreqDist: frequency distribution\n",
    "    \"\"\"\n",
    "    tokens = nltk.regexp_tokenize(doc,'\\S+')\n",
    "    filtered_tokens = [w.lower().strip('.,?!\"\\'') for w in tokens]\n",
    "    consolidated_tokens = []\n",
    "    for w in filtered_tokens:\n",
    "        if isNumber(w):\n",
    "            consolidated_tokens.append(\"<NUMBER>\")\n",
    "            continue            \n",
    "        elif re.match(\"[\\d]+(pm|am)$\", w):\n",
    "            consolidated_tokens.append(\"<TIME>\")\n",
    "            continue\n",
    "        elif re.match(\"[\\d]+:[\\d]+(pm|am)?$\", w):\n",
    "            consolidated_tokens.append(\"<TIME>\")\n",
    "            continue\n",
    "        elif re.match(\"\\(?(\\w+)\\)?$\", w):\n",
    "            m = re.match(\"\\(?(\\w+)\\)?$\", w)\n",
    "            consolidated_tokens.append(m.group(1))\n",
    "            continue\n",
    "        else:\n",
    "            consolidated_tokens.append(w) \n",
    "    \n",
    "    if stopwords:\n",
    "        consolidated_tokens = [w for w in consolidated_tokens if w not in STOPWORDS and w != \"\" ]\n",
    "    else:\n",
    "        consolidated_tokens = [w for w in consolidated_tokens if w != \"\"]\n",
    "             \n",
    "    fd = nltk.FreqDist(consolidated_tokens)\n",
    "    return fd\n",
    "\n",
    "def computeUnigramDistribution(doc: str, n_words: int = None, stopwords: bool = False) -> T.Tuple[dict, float]:\n",
    "    \"\"\"\n",
    "    Computes the relative frequencies (i.e., probs) of the most common unigrams\n",
    "        in a document\n",
    "\n",
    "    Args:\n",
    "        doc (str): string containing the entire document\n",
    "        n_words (int, optional): Number of most common words to consider.\n",
    "            Defaults to None.\n",
    "        stopwords: boolean flag indicating whether or not\n",
    "            to remove the stopwords from the sentence. True indicates\n",
    "            to remove the stopwords.\n",
    "\n",
    "    Returns:\n",
    "        dict: relative frequencies of the form dist[word] = prob\n",
    "        float: sum of all the probabilities of the n_words most frequent unigrams\n",
    "    \"\"\"\n",
    "    fd = computeFreqDistribution(doc, stopwords)\n",
    "    keys = list(fd.keys())[:n_words]\n",
    "    values = list(fd.values())[:n_words]\n",
    "    N = float(sum(values))\n",
    "    dist = {}\n",
    "    for key in keys:\n",
    "        dist[key] = float(fd[key])/N\n",
    "    return (dist,N)\n",
    "\n",
    "def mergeDistributionJS(dist1: dict, dist2: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Merges the two distributions used in the JS divergence\n",
    "\n",
    "    Args:\n",
    "        dist1 (dict): probability distribution of the form dist1[word] = prob\n",
    "        dist2 (dict): probability distribution of the form dist2[word] = prob\n",
    "\n",
    "    Returns:\n",
    "        dict: New merged distribution including all words from both distributions\n",
    "    \"\"\"\n",
    "    mergeDist = {}\n",
    "    for key in dist1.keys():\n",
    "        mergeDist[key] = 1/2*dist1[key]\n",
    "    for key in dist2.keys():\n",
    "        if key in mergeDist.keys():\n",
    "            mergeDist[key] += 1/2*dist2[key]\n",
    "        else:\n",
    "            mergeDist[key] = 1/2*dist2[key]\n",
    "    return mergeDist\n",
    "\n",
    "def KLDivergence(P: dict, M: dict, log_base: float = math.e) -> float:\n",
    "    \"\"\"\n",
    "    Computes the KL divergence for two distributions\n",
    "        KL(P||M) = \\sum_{x \\in X}[p(x) * \\log(p(x)/q(x))]\n",
    "\n",
    "    Args:\n",
    "        P (dict): probability distribution of words\n",
    "        M (dict): probability distribution of words\n",
    "        log_base (float): Base value to use for log.\n",
    "            Defaults to Euler's constant\n",
    "\n",
    "    Returns:\n",
    "        float: KL divergence of two distributions\n",
    "    \"\"\"\n",
    "    div = 0\n",
    "    for key in P.keys():\n",
    "        div += P[key] * math.log(P[key] / M[key], log_base)\n",
    "    return div\n",
    "\n",
    "def JSDivergence(doc1: str, doc2: str, num_words: int = None, log_base: float = math.e, stopwords: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the JS Divergence value for two corpora\n",
    "\n",
    "    Args:\n",
    "        doc1 (str): string containing the entire document\n",
    "        doc2 (str): string containing the entire document\n",
    "        num_words (int): number of most frequent words to\n",
    "            consider. Defaults to all words.\n",
    "        log_base (float): Base value to use for log.\n",
    "            Defaults to Euler's constant\n",
    "        stopwords (bool): boolean flag indicating whether or not\n",
    "            to remove the stopwords from the sentence. True indicates\n",
    "            to remove the stopwords.\n",
    "\n",
    "    Returns:\n",
    "        float: the JS divergence of the two corpora\n",
    "    \"\"\"\n",
    "    P, N1 = computeUnigramDistribution(doc1, num_words, stopwords)\n",
    "    Q, N2 = computeUnigramDistribution(doc2, num_words, stopwords)\n",
    "    M = mergeDistributionJS(P, Q)\n",
    "    js = 1/2*KLDivergence(P, M, log_base) + 1/2*KLDivergence(Q, M, log_base)\n",
    "    return js / math.log(log_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f2b8ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_doc(batch):\n",
    "    doc = \" \".join(batch)\n",
    "    return doc\n",
    "\n",
    "\n",
    "def open_text_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r',encoding=\"utf8\") as file:\n",
    "            input_texts = file.readlines()\n",
    "        return input_texts\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found.\")\n",
    "    except IOError:\n",
    "        print(\"Error reading the file.\")\n",
    "\n",
    "def create_batches_with_replacement(df, num_batches, batch_size):    \n",
    "    batches = []\n",
    "    for _ in range(num_batches):\n",
    "        batch = df.sample(n=batch_size)\n",
    "        batches.append(batch)\n",
    "    \n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00073ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the dataset file will have both languages, give the column name of the\n",
    "#language you want to compare against the development set\n",
    "#give the column name of the development set language\n",
    "COLUMN_NAME = \"<COLUMN NAME OF THE LANGUAGE YOU WANT TO CALCULATE JS DIV>\"\n",
    "#read the datafram containing the data \n",
    "#the data should contain both the source and the target language\n",
    "df = pd.read_csv(\"<PATH TO THE CSV FILE OF THE DATASET>\")\n",
    "#here we load the development set which we want to compare against the sampled sentences\n",
    "with open(\"<PATH TO THE DEVELOPMENT SET>\", 'r',encoding=\"utf8\") as file:\n",
    "    es_eval_sentences = file.readlines()\n",
    "\n",
    "#we can make the evaluation set into a doc as required by the JS divergence function\n",
    "es_doc_eval = make_doc(es_eval_sentences)\n",
    "\n",
    "#we create the batches by sampling our dataset\n",
    "batches = create_batches_with_replacement(df, num_batches, batch_size)\n",
    "batches = [(num,batch) for num,batch in enumerate(batches)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7e4588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We calculate the JS divergence for each batch and store it in a dictionary\n",
    "data = {\"batch_num\":[],\n",
    "       \"Js_div\":[]}\n",
    "for num,batch in tqdm(batches):\n",
    "    train_doc = make_doc(list(batch[COLUMN_NAME].values))\n",
    "    score = JSDivergence(train_doc,es_doc_eval,stopwords=True)\n",
    "    data[\"batch_num\"].append(num)\n",
    "    data[\"Js_div\"].append(score) \n",
    "\n",
    "#we create a dataframe with the results and sort it by the JS divergence\n",
    "js_df = pd.DataFrame(data=data)\n",
    "js_df.sort_values(by=\"Js_div\",ascending=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc35996",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we create a final dataframe with the sentences that we want to keep\n",
    "#based on the JS divergence and the desired size of the final dataset\n",
    "final_df = pd.DataFrame()\n",
    "i=0\n",
    "while True:\n",
    "    print(i)\n",
    "    batch_num = js_df.iloc[i,0]\n",
    "    target_df = batches[batch_num][1]\n",
    "    assert batches[batch_num][0]==batch_num\n",
    "    final_df = pd.concat([final_df,target_df])\n",
    "    final_df.drop_duplicates(subset=[\"es\"],inplace=True)\n",
    "    print(final_df.shape)\n",
    "    if len(final_df) >= target_size:\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf747bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"<PATH TO THE LOCATION OF THE FINAL OUTPUT>\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb16b199",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
